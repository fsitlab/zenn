# 追記候補：概念補完メモ
*元記事への組み込み用。骨子2026-02-21 rev2 に対する差分。*

---

## 追記A　第0章への追加：技術要素マスターマッピング表

> **元記事の対応章**: 第0章（全体地図：3層モデル）
> **追記 or 変更**: 追記
> **理由**: 現状、3層モデルの定義はあるが「各技術が具体的にどの層か」の早見表がない。読者が各章を読む前に全体を俯瞰できる表を第0章末尾に置く。

**概要**: 各技術要素が①②③のどの層に属するかを一覧化した参照表。この表が記事全体の索引として機能する。

| 技術要素 | ①入出力層 | ②ホスト制御層 | ③プロンプト層 | 備考 |
|--|--|--|--|--|
| temperature / top_p 等 | ✅ | | | JSONパラメータ |
| max_tokens / stop | ✅ | | | |
| Extended Thinking | ✅ | | | `thinking`ブロックとして出力 |
| Prompt Caching | ✅ | | | `cache_control`タグで制御 |
| Batch API | ✅ | | | 非同期実行。通常APIと別エンドポイント |
| tool_use定義（tools[]） | ✅ | | | |
| tool_use実行 | | ✅ | | LLMは実行しない |
| Computer Use（tool） | ✅（定義） | ✅（実行） | | スクリーンショットはtool_resultで①層に戻る |
| MCPサーバー | | ✅ | | ツール実装・公開 |
| MCPクライアント | | ✅ | | tools[]へ変換する橋渡し |
| `.mcp.json` | | ✅（設定） | | |
| Hooks | | ✅ | | |
| Permissions / permissionMode | | ✅ | | |
| ベクトルDB / 検索エンジン | | ✅ | | RAG②層コンポーネント |
| CLAUDE.md / AGENTS.md | | | ✅ | system prompt注入 |
| Skill（ロード後） | | | ✅ | |
| RAG結果（従来型） | | | ✅ | ホストが機械的に注入 |
| RAG結果（Tool型） | ✅（tool_result） | ✅（実行） | | LLMが能動的に呼ぶ |
| Subagent（Taskツール呼び出し） | ✅ | ✅ | ✅ | 3層全てを含む |
| Agent間通信（A2A） | ✅ | ✅ | | JSON-RPC 2.0 |

---

## 追記B　Extended Thinking / Reasoning Tokens

> **元記事の対応章**: 第1章（LLMへの入出力）、第1-1の各社APIフォーマット内
> **追記 or 変更**: 追記
> **理由**: ①層のパラメータとして現状未記載。思考プロセス自体がJSONブロックとして返るという構造的な特異点であり、概念地図として欠落している。

> **対象層**: ① LLM入出力層

**概要**: LLMに「答える前に考える」ステップを与える機能。思考内容が`thinking`ブロックとして出力に含まれ、ホストはそれを次のリクエストに引き継げる。通常のtextブロックと異なり、**思考ブロックは編集不可**（改ざん防止）。

```json
// Anthropicリクエスト: thinkingを有効化
{
  "model": "claude-sonnet-4-6",
  "max_tokens": 16000,
  "thinking": {"type": "enabled", "budget_tokens": 10000},
  "messages": [{"role": "user", "content": "この問題を解いて"}]
}

// レスポンス: thinkingブロック + textブロック
{
  "content": [
    {"type": "thinking", "thinking": "まず条件を整理すると..."},
    {"type": "text", "text": "答えは42です"}
  ]
}
```

**各社の対応**:
| | 実装 | パラメータ |
|--|--|--|
| Anthropic | Extended Thinking | `thinking: {type: "enabled", budget_tokens: N}` |
| OpenAI | Reasoning（o1/o3系） | `reasoning_effort: "high/medium/low"` |
| Gemini | Thinking（Gemini 2.0 Flash Thinking等） | `thinkingConfig: {thinkingBudget: N}` |

- 📘 [Anthropic Extended Thinking ガイド](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking)
- 📘 [OpenAI Reasoning Models ガイド](https://platform.openai.com/docs/guides/reasoning)
- 📘 [Gemini Thinking ガイド](https://ai.google.dev/gemini-api/docs/thinking)

---

## 追記C　Prompt Caching

> **元記事の対応章**: 第1章（LLMへの入出力）、第1-1の各社APIフォーマット内
> **追記 or 変更**: 追記
> **理由**: ①層のパラメータとして現状未記載。コスト・レイテンシに直結し、長いsystem promptや大量のドキュメントを繰り返し送る場面で頻繁に使われる。③層（CLAUDE.md、Skill）との連携で理解が深まる。

> **対象層**: ① LLM入出力層（キャッシュ有効化の宣言）/ ② ホスト制御層（キャッシュ管理はサーバー側）

**概要**: 繰り返し使うコンテキスト（system prompt・大量ドキュメント等）をサーバー側にキャッシュし、2回目以降の入力トークン料金を削減する機能。`cache_control`タグをJSONに追加するだけで有効化。

```json
// Anthropic: cache_controlを追加
{
  "system": [
    {"type": "text", "text": "長大なsystem prompt...",
     "cache_control": {"type": "ephemeral"}}
  ],
  "messages": [{"role": "user", "content": "質問"}]
}
```

**各社の対応**:
| | 実装 | キャッシュ期間 |
|--|--|--|
| Anthropic | Prompt Caching | 5分（ephemeral） |
| OpenAI | Prompt Caching | 自動（明示不要）|
| Gemini | Context Caching | 明示的TTL指定 |

- 📘 [Anthropic Prompt Caching ガイド](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching)
- 📘 [OpenAI Prompt Caching ガイド](https://platform.openai.com/docs/guides/prompt-caching)
- 📘 [Gemini Context Caching ガイド](https://ai.google.dev/gemini-api/docs/caching)

---

## 追記D　Batch API（非同期バッチ処理）

> **元記事の対応章**: 第1章（LLMへの入出力）
> **追記 or 変更**: 追記
> **理由**: ①層の通信プロトコルとして未記載。通常のリクエスト（同期SSE）と対比させることで「LLMへの入出力の全形態」が揃う。

> **対象層**: ① LLM入出力層

**概要**: 複数のリクエストをまとめて非同期で送り、数時間後に結果を取得する仕組み。通常APIより最大50%安い。評価・大量文書処理など、即時性が不要なタスクに使う。

```json
// Anthropicバッチリクエスト（複数をまとめて送信）
POST /v1/messages/batches
{
  "requests": [
    {"custom_id": "req-1", "params": {"model": "...", "messages": [...]}},
    {"custom_id": "req-2", "params": {"model": "...", "messages": [...]}}
  ]
}
// → batch_id を受け取り、後でポーリングして結果取得
```

| | Anthropic | OpenAI |
|--|--|--|
| エンドポイント | `/v1/messages/batches` | `/v1/batches` |
| 最大遅延 | 24時間 | 24時間 |
| 割引 | 最大50% | 50% |

- 📘 [Anthropic Batch API ガイド](https://docs.anthropic.com/en/docs/build-with-claude/message-batches)
- 📘 [OpenAI Batch API ガイド](https://platform.openai.com/docs/guides/batch)

---

## 追記E　Computer Use

> **元記事の対応章**: 第2章（Tool Use）、第2-1の各社Tool Use実装内
> **追記 or 変更**: 追記（第2章内の独立項目として）
> **理由**: マスターマッピング表（追記A）で①②両層にまたがると示したが、本文中に詳細がない。スクリーンショットがtool_resultとして①層に戻るという構造は、通常のtool_useと異なる特異点。

> **対象層**: ① LLM入出力層（tool_use定義・tool_result受け取り）/ ② ホスト制御層（実際の画面操作）

**概要**: `computer`・`bash`・`text_editor`等のツールを通じてLLMがGUI/CUIを直接操作する仕組み。通常のtool_useと構造は同じだが、**ツール結果がスクリーンショット（画像）**として①層に戻る点が特徴。ホスト側が仮想環境を用意し、LLMの指示を実行する責任を持つ。

```json
// レスポンス: computer tool_useブロック
{"type": "tool_use", "name": "computer",
 "input": {"action": "screenshot"}}

// 次リクエスト: スクリーンショットをtool_resultで返す（画像として①層に入力）
{"type": "tool_result", "tool_use_id": "...",
 "content": [{"type": "image", "source": {"type": "base64", "data": "..."}}]}
```

- 📘 [Anthropic Computer Use ガイド](https://docs.anthropic.com/en/docs/build-with-claude/computer-use)

---

## 追記F　Context Window とトークン管理

> **元記事の対応章**: 第1章（LLMへの入出力）または第0章の補足
> **追記 or 変更**: 追記
> **理由**: ③層の設計（CLAUDE.md、Skill、RAG）はすべてContext Windowの有限性を前提にしている。概念地図として「なぜSkillは遅延ロードするのか」「なぜRAGで全文を入れないのか」の根拠となる制約が未記載。

> **対象層**: ① LLM入出力層の制約（すべての層に影響）

**概要**: LLMが一度に処理できるトークン数の上限。`messages[]`の全履歴 + system prompt + tools定義 + 出力予約がこの枠に収まる必要がある。**Skillの遅延ロード（第6章）・RAGの選択的注入（第4章）・Subagentへの分散（第8章）は、すべてContext Windowの制約への対応策**。

| モデル | Context Window（入力上限） |
|--|--|
| Claude claude-sonnet-4-6 | 200K tokens |
| GPT-4o | 128K tokens |
| Gemini 2.0 Flash | 1M tokens |

**コスト構造との関係**: 入力トークン数 × 単価。長いsystem promptを毎回送ると費用増。→ Prompt Caching（追記C）が対策。

- 📘 [Anthropic モデル一覧（context window付き）](https://docs.anthropic.com/en/docs/about-claude/models)

---

## 追記G　ベクトルDB / 検索エンジン（RAGの②層コンポーネント）

> **元記事の対応章**: 第4章（RAG）
> **追記 or 変更**: 追記
> **理由**: 第4章で「従来RAGは③層」と整理されているが、実際にベクトル検索を実行するコンポーネントは②層に属する。ベクトルDBがどの層に位置するかが未記載で、概念地図として不完全。

> **対象層**: ② ホスト制御層

**概要**: ドキュメントを数値ベクトルに変換して格納し、意味的類似度で検索するデータベース。**RAGの「取得」部分を担当するのは②層のホストプログラムであり、LLMでも③層のプロンプトでもない**。

```
従来RAG の処理フロー（全て②層が担当）:
ユーザー入力
  → Embedding Model（②層）でベクトル化
  → ベクトルDB（②層）でANN検索
  → 上位k件のテキストを取得
  → system promptに注入（③層へ）
  → LLMへ送信（①層へ）
```

**主要ベクトルDB**:

| 種別 | 代表例 |
|--|--|
| マネージドクラウド | Pinecone、Weaviate Cloud、Qdrant Cloud |
| セルフホスト | Chroma、Milvus、Qdrant |
| PostgreSQL拡張 | pgvector |
| 全文検索兼用 | Elasticsearch、OpenSearch |

- 📘 [LlamaIndex Vector Store ドキュメント](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/)
- 📘 [LangChain Vector Store ドキュメント](https://python.langchain.com/docs/concepts/vectorstores/)

---

## 追記H　Hooks 全イベント一覧（第7章の補完）

> **元記事の対応章**: 第7章（Hooks）
> **追記 or 変更**: 変更（「主要」→全量に差し替え）
> **理由**: 現状「14種のイベント一覧」と書きながら6種しか示されていない。網羅を目的とする記事として不整合。全量を示すか、「代表的なもの」と明示するか、どちらかに統一する。

> **対象層**: ② ホスト制御層

**Claude Code Hooks イベント一覧**（2026-02-21 時点、公式ドキュメントより）:

| イベント | タイミング | ブロック可否 |
|--|--|--|
| `PreToolUse` | tool_use受け取り直後、実行前 | ✅（exit 2） |
| `PostToolUse` | ツール実行後 | ❌ |
| `PreCompact` | コンテキスト圧縮前 | ✅ |
| `PostCompact` | コンテキスト圧縮後 | ❌ |
| `SessionStart` | セッション開始時 | ❌ |
| `SessionEnd` | セッション終了時 | ❌ |
| `UserPromptSubmit` | ユーザー入力送信後 | ✅ |
| `PreBashExecute` | Bashコマンド実行前 | ✅ |
| `PostBashExecute` | Bashコマンド実行後 | ❌ |
| `SubagentStart` | Subagent起動時 | ✅ |
| `SubagentStop` | Subagent停止時 | ❌ |
| `FileWrite` | ファイル書き込み前 | ✅ |
| `FileRead` | ファイル読み込み後 | ❌ |
| `Notification` | 通知送信時 | ❌ |

※ イベント名・数は公式ドキュメントで随時更新される可能性あり。

- 📘 [Claude Code Hooks 公式ドキュメント](https://code.claude.com/docs/en/hooks)（常に最新を参照）

---

## 追記I　Structured Output / JSON Mode の層の整理

> **元記事の対応章**: 第1章 1-7（構造化出力 / JSON Schema）
> **追記 or 変更**: 変更（リンク集から概念説明に昇格）
> **理由**: 現状リンクのみで概念説明がない。「LLMの出力を構造化する」手段が①層のAPIレベル強制と③層のプロンプト指示の2種類あることが未整理。この違いは「なぜJSON Modeでも壊れることがあるか」の理解に直結する。

> **対象層**: ① LLM入出力層（API強制）/ ③ プロンプト層（プロンプト指示）

**概要**: LLMの出力を特定のJSONスキーマに従わせる仕組み。**①層でAPIが文法的に保証する方法**と、**③層でプロンプトが「JSON形式で答えて」と指示する方法**の2つがある。前者は構文エラーがないことを保証するが、値の意味的正しさは保証しない。

| 方式 | 層 | 構文保証 | 値の意味保証 |
|--|--|--|--|
| `response_format: {type: "json_object"}` (OpenAI) | ① | ✅ | ❌ |
| `response_format: {type: "json_schema", schema: {...}}` (OpenAI) | ① | ✅（スキーマ準拠） | ❌ |
| プロンプトで「JSON形式で答えて」 | ③ | ❌ | ❌ |
| Anthropicの`tools[]`を構造化出力に流用 | ① | ✅（input_schema準拠） | ❌ |

**Anthropicにおける構造化出力**: 専用の`response_format`パラメータは現状なし。`tools[]`の`input_schema`を使ってLLMに強制的に構造化JSONを返させるパターンが標準的。

- 📘 [OpenAI Structured Outputs ガイド](https://platform.openai.com/docs/guides/structured-outputs)
- 📘 [Anthropic Tool Use による構造化出力パターン](https://docs.anthropic.com/en/docs/build-with-claude/tool-use#json-mode)
- 🔗 [Instructor - Mode Comparison Guide](https://python.useinstructor.com/modes-comparison/)（元記事記載済み）

---

## 追記J　第0章への追加：3層判定基準

> **元記事の対応章**: 第0章（全体地図：3層モデル）
> **追記 or 変更**: 追記
> **理由**: 記事公開後に新しい技術が出たとき、読者が自力でこの地図にマッピングできるようにするため。参照記事としての賞味期限を延ばす。

**新技術を見たときの層判定フロー**:

```
Q1. LLMへのリクエスト/レスポンスJSONに、
    この概念が直接フィールドとして現れるか？
    → YES: ①層

Q2. プログラムが「必ず実行する」を保証するか？
    （LLMの確率的判断を介さずに動くか？）
    → YES: ②層

Q3. system promptまたはmessages[]内のテキストとして
    展開されるか？
    → YES: ③層

※ 複数のYES: 複数層にまたがる（Subagent、Tool型RAGなど）
```

---

*作成日: 2026-02-21*
*対象: 骨子バージョン 2026-02-21 rev2 への差分*
