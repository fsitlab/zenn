# 追記A（改訂版）：第0章への追加 ── 4層モデルと技術要素マスターマッピング表

> **元記事の対応章**: 第0章（全体地図：3層モデル）
> **追記 or 変更**: 変更（3層→4層モデルへ改訂）
> **理由**: 「②ホスト制御層」に粒度の異なるものが混在している（Hooksと VS Codeが同居）。アプリ層を独立させることで「誰がLLMと話すか」「誰がユーザーと話すか」の責任境界が明確になる。

---

## 4層モデル（改訂版）

```
④ アプリ層       ユーザーと対話する完全外部アプリ。①②③を使って動く。
                  例: VS Code / Kiro / Dify / Claude.ai / 従来型RAGパイプライン

② ホスト制御層   LLMを直接呼び出すランタイム。Tool Use実行・Hooks・権限管理を担う。
                  プログラムが保証する決定論的制御。

③ プロンプト層   LLMへの入力テキストを構成・操作する層。
                  コードではなく「言葉」で動作を制御する。

① LLM入出力層   LLMとの通信プロトコルの実態。
                  モデル名・ロール・コンテキスト（tools/content）・パラメータのJSON構造。
                  LLMはここでJSONを受け取り、JSONを返すだけ。実行しない。
```

---

## 4層の判定基準（新技術が出たときの自己診断フロー）

```
Q1. ユーザーやシステム管理者が直接触るアプリか？
    LLMと直接JSON通信せず、②③①を通じて動くか？
    → YES: ④層

Q2. LLMへのリクエスト/レスポンスJSONを直接生成・処理するか？
    プログラムが「必ず実行する」を保証する制御か？
    → YES: ②層

Q3. system promptまたはmessages[]内のテキストとして展開されるか？
    → YES: ③層

Q4. LLMへのリクエスト/レスポンスJSONのフィールド構造そのものか？
    （model名、role、content[]ブロック、パラメータ等）
    → YES: ①層

※ 複数のYES: 複数層にまたがる（後述の注を参照）
```

---

## 技術要素マスターマッピング表（4層版）

| 技術要素 | ④アプリ層 | ②ホスト制御層 | ③プロンプト層 | ①LLM入出力層 |
|--|--|--|--|--|
| **──① 入出力フォーマット──** |
| model / role / max_tokens 等 | | | | ✅ |
| content[]ブロック（text/image/tool_use/tool_result） | | | | ✅ |
| Extended Thinking（thinkingブロック） | | | | ✅ |
| Prompt Caching（cache_control） | | | | ✅ |
| Batch API | | | | ✅ |
| Structured Output / JSON Schema | | | | ✅ |
| ストリーミング（SSE / WebSocket） | | | | ✅ |
| **──② ホスト制御──** |
| tool_use 実行 | | ✅ | | |
| Computer Use（実行側） | | ✅ | | |
| MCPサーバー | | ✅ | | |
| MCPクライアント（tools[]変換） | | ✅ | | |
| `.mcp.json` | | ✅ | | |
| Hooks | | ✅ | | |
| Permissions / permissionMode | | ✅ | | |
| **──③ プロンプト──** |
| CLAUDE.md / AGENTS.md | | | ✅ | |
| Skill（ロード後） | | | ✅ | |
| RAG結果注入（従来型） | | | ✅ | |
| **──④ アプリ──** |
| VS Code / Kiro 等のIDE | ✅ | | | |
| 従来型RAGパイプライン全体 | ✅ | | | |
| ベクトルDB / 検索エンジン | ✅ | | | |
| Dify / n8n（ノーコード基盤） | ✅ | | | |
| **──複数層にまたがるもの──** |
| tool_use定義（tools[]） | | | | ✅（JSON構造として） |
| Computer Use（tool_result含む） | | ✅ | | ✅ |
| MCPサーバー（起動設定） | | ✅ | ✅ | |
| RAG（Tool型） | | ✅（実行） | | ✅（tool_result） |
| LangChain / LlamaIndex | ✅（アプリ視点） | ✅（LLM呼び出し担当） | | |
| Subagent / Agent | ✅ | ✅ | ✅ | ✅ |

---

## 注：④層と②層の境界について

LangChain・Difyのように「④層のアプリでありながら、内部でLLM APIを直接叩く（②層の機能も持つ）」ものが存在する。

**判断軸**:
- ユーザーやシステムが「このフレームワークを使って構築する」視点 → **④層**
- そのフレームワークが「LLMのtool_use実行を担う」視点 → **②層**

同一ソフトウェアが複数層にまたがること自体は問題ではなく、「**今どの層の話をしているか**」を意識することが重要。

---

## ④層がない場合と比較した整理効果

| 技術 | 3層モデルでの位置 | 4層モデルでの位置 | 改善点 |
|--|--|--|--|
| VS Code + 拡張 | ②（ホスト制御） | ④（アプリ） | HooksやPermissionsと分離 |
| 従来型RAGパイプライン | ②（ホスト制御） | ④（アプリ） | ベクトルDB実行者として明確化 |
| Dify | ②（ホスト制御） | ④（アプリ）/ ②（LLM呼び出し） | 両面を使い分けて説明可能 |
| AI Agentフレームワーク | ②（ホスト制御） | ④（アプリ）/ ②（実行） | オーケストレーターとしての位置が明確 |

---

*改訂日: 2026-02-21*
*対象: 骨子バージョン 2026-02-21 rev2 への差分（追記A改訂版）*
